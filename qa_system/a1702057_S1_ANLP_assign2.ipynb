{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# Read the dataset\n",
        "news_df = pd.read_csv('news_dataset.csv', encoding='latin1')\n",
        "\n",
        "# Data cleaning and preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "news_df['cleaned_article'] = news_df['article'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "8ZyI0beWPQHF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the pre-trained QA model and tokenizer\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL2u2ztuPSuG",
        "outputId": "3431cb9e-d4a7-40e2-90db-91c3e22ad4b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def answer_question(question, article):\n",
        "    # Encode the question and article\n",
        "    encoded_input = tokenizer(question, article, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    # Get the model output\n",
        "    output = model(**encoded_input)\n",
        "    # Extract the answer and confidence scores\n",
        "    answer_start = output.start_logits.argmax()\n",
        "    answer_end = output.end_logits.argmax()\n",
        "    answer = tokenizer.decode(encoded_input[\"input_ids\"][0][answer_start:answer_end+1])\n",
        "    # Set confidence threshold\n",
        "    confidence_threshold = 0.5\n",
        "    # Calculate the confidence score\n",
        "    start_prob = torch.softmax(output.start_logits, dim=1)[0][answer_start].item()\n",
        "    end_prob = torch.softmax(output.end_logits, dim=1)[0][answer_end].item()\n",
        "    confidence_score = (start_prob + end_prob) / 2\n",
        "    if confidence_score >= confidence_threshold:\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No answer found.\""
      ],
      "metadata": {
        "id": "IO1GDNCTPT_C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test\n",
        "question = \"Who is the vice chairman of Samsung\"\n",
        "article_id = 17307\n",
        "article = news_df[news_df['id'] == article_id]['cleaned_article'].iloc[0]\n",
        "answer = answer_question(question, article)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Article ID: {article_id}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEN1NjjcPYe-",
        "outputId": "f33bde5e-b5b3-48ce-b100-16e455690ba3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who is the vice chairman of Samsung\n",
            "Article ID: 17307\n",
            "Answer: No answer found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read test questions and answers\n",
        "test_data = []\n",
        "with open('test_questions_students_contributed.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        match = re.search(r\"\\('(.*?)', (\\d+)\\) \\('(.*?)', \\d+\\)\", line)\n",
        "        if match:\n",
        "            question = match.group(1)\n",
        "            article_id = int(match.group(2))\n",
        "            answer = match.group(3)\n",
        "            test_data.append((question, article_id, answer))\n",
        "\n",
        "# Evaluate the QA system\n",
        "correct_answers = 0\n",
        "for question, article_id, true_answer in test_data:\n",
        "    article = news_df[news_df['id'] == article_id]['cleaned_article'].iloc[0]\n",
        "    predicted_answer = answer_question(question, article)\n",
        "    # Print question, predicted answer, and true answer\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Predicted Answer: {predicted_answer}\")\n",
        "    print(f\"True Answer: {true_answer}\")\n",
        "    if predicted_answer.lower() == true_answer.lower():\n",
        "        correct_answers += 1\n",
        "        print(\"Result: Correct\\n\")\n",
        "    else:\n",
        "        print(\"Result: Incorrect\\n\")\n",
        "\n",
        "accuracy = correct_answers / len(test_data)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vTcGBtNPtz8",
        "outputId": "e74bbb15-46be-4995-a741-cfef32950e70"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who was the President during the conflict?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: George W. Bush\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is the Senator of Colorado?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Cory Gardner\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What was the revolt?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Tea Party Revolt\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did they get control back?\n",
            "Predicted Answer: 2006\n",
            "True Answer: 2010\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Where is the senior Republican from?\n",
            "Predicted Answer: oklahoma\n",
            "True Answer: Oklahoma\n",
            "Result: Correct\n",
            "\n",
            "Question: Who was the president during the Iraq War?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: George W. Bush\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What amount did Fox News offer?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: 20 Million\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Where did Charlie Rose interview Kelly?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: CBS Sunday Morning\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did Andrew Lack take over?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: 2015\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What is Andrew Lack adding?\n",
            "Predicted Answer: a journalist\n",
            "True Answer: journalist schooled in the preferences and worldviews of the conservative Americans who helped elect Mr Trump\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When is Donald Trump inaugurated?\n",
            "Predicted Answer: jan 20\n",
            "True Answer: Jan. 20\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is a executive chairman?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Rupert Murdoch\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is an executive chairman of 21st Century Fox?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Rupert Murdoch\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is a novelist?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Douglas Brunt\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Where was the shooting?\n",
            "Predicted Answer: panzhihua\n",
            "True Answer: Panzhihua\n",
            "Result: Correct\n",
            "\n",
            "Question: Who was embarrassed by the violence?\n",
            "Predicted Answer: president xi jinping\n",
            "True Answer: Xi Jinping\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Chen Zhongshu is the head of what?\n",
            "Predicted Answer: panzhihua land and resources bureau\n",
            "True Answer: Panzhihua Land and Resources Bureau,\n",
            "Result: Incorrect\n",
            "\n",
            "Question: The shooting happened where?\n",
            "Predicted Answer: panzhihua\n",
            "True Answer: Panzhihua\n",
            "Result: Correct\n",
            "\n",
            "Question: What was Chen Zhongshu the head of?\n",
            "Predicted Answer: panzhihua land and resources bureau\n",
            "True Answer: Panzhihua Land and Resources Bureau,\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did Zhang starting working there?\n",
            "Predicted Answer: 2006\n",
            "True Answer: 2006\n",
            "Result: Correct\n",
            "\n",
            "Question: Where is Panzhihua?\n",
            "Predicted Answer: an industrial city in sichuan province\n",
            "True Answer: Sichuan Province\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What is Panzhihua?\n",
            "Predicted Answer: an industrial city in sichuan province\n",
            "True Answer: industrial city\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When where they murdered?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: June 2015\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What did they find him guilty of?\n",
            "Predicted Answer: hate crimes resulting in death obstruction of religion and firearms violations\n",
            "True Answer: hate crimes resulting in death, obstruction of religion and firearms violations\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who faces the death penalty?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Dylan Roof\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who faces life in prison?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Dylan Roof\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did he write?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Dec. 16\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is evil?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Dylan Roof\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did the police officer shoot?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: 2015\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When was the coalition made?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: 2014\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What did the Islamic State seize?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: large areas in Iraq and neighboring Syria\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did the car bombing happen?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Monday\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Where did the bombing occur?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: a busy Baghdad market\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Where did the attack occur?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Sadr City\n",
            "Result: Incorrect\n",
            "\n",
            "Question: How many died?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: 36\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What was repeatedly attacked after the invasion?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: Sadr City\n",
            "Result: Incorrect\n",
            "\n",
            "Question: What did the gunman have?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: a rifle\n",
            "Result: Incorrect\n",
            "\n",
            "Question: Who is the spokesman?\n",
            "Predicted Answer: numan kurtulmus\n",
            "True Answer: Numan Kurtulmus\n",
            "Result: Correct\n",
            "\n",
            "Question: Where is the gunman from?\n",
            "Predicted Answer: kyrgyzstan or elsewhere in central asia\n",
            "True Answer: Kyrgyzstan or elsewhere in Central Asia\n",
            "Result: Correct\n",
            "\n",
            "Question: What happened in November?\n",
            "Predicted Answer: No answer found.\n",
            "True Answer: A car bombing\n",
            "Result: Incorrect\n",
            "\n",
            "Question: When did Kurtulmus say the attack happened?\n",
            "Predicted Answer: after midnight on sunday morning\n",
            "True Answer: just after midnight on Sunday morning\n",
            "Result: Incorrect\n",
            "\n",
            "Accuracy: 0.15\n"
          ]
        }
      ]
    }
  ]
}