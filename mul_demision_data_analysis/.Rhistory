plot(cities_sf_small, pch = 16, col = "blue", cex = 0.1, add = TRUE)
plot(cities_sf_large, pch = 16, col = "red", cex = 1, add = TRUE)
aus_shape <- vect("SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp")
aus_shape <- aus_shape[!is.na(aus_shape$AREASQKM21)]
library(RColorBrewer)
sa_shape <- aus_shape[aus_shape$STE_NAME21 == "South Australia"]
sa_shape_4 <- aggregate(sa_shape, by = 'SA4_NAME21')
par(mar = c(1, 1, 1, 1))
options(repr.plot.width = 20, repr.plot.height = 20)
color_setting <- brewer.pal(length(unique(sa_shape_4$SA4_NAME21)), "Set1")
plot(sa_shape_4, col = color_setting)
plot(sa_shape, add = TRUE)
legend("top", legend = unique(sa_shape_4$SA4_NAME21), fill = color_setting, title = "statistical areas level 4", cex = 0.75)
library(terra)
greater_adelaide <- aus_shape[aus_shape$GCC_NAME21 == "Greater Adelaide"]
plot(greater_adelaide)
# Load crime data
crime_data <- read.csv('crimeCounts.csv')
nrow(crime_data)
crime_data <- na.omit(crime_data)
nrow(crime_data)
colnames(crime_data) <- c('id', 'suburb', 'crimeCounts')
colnames(crime_data)
salisbury <- sa_shape[sa_shape$SA3_NAME21 == "Salisbury"]
suburb_crime_count <- left_join(
data.frame("suburb" = tolower(salisbury$SA2_NAME21)),
crime_data %>% mutate(suburb = tolower(suburb)),
by = "suburb"
)
salisbury$crimeCounts <- suburb_crime_count$crimeCounts
salisbury_raster <- rast(
ncol = 1000, nrow = 1000,
xmin = 138.50, xmax = 138.71,
ymin = -34.84, ymax = -34.69
)
salisbury_raster <- rasterize(salisbury, salisbury_raster, "crimeCounts")
terra::plot(
salisbury_raster,
col = brewer.pal(10, "Reds")
)
terra::lines(salisbury)
text(salisbury, salisbury$SA2_NAME21, inside = TRUE, cex = 0.6, col = "Black")
set.seed(123)
#Create a Window_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Window_Kmeans = DSC_TwoStage(micro = DSC_Window(horizon = 200), macro = DSC_Kmeans(k = 5))
# Get 500 data points from the data stream through the update function and use them to update the Window_Kmeans object
update(Window_Kmeans, stream, n = 500)
#Print the Window_Kmeans object to view the clustering results
Window_Kmeans
# Draw clustering visualization of Window_Kmeans objects
plot(Window_Kmeans, stream)
# Use the evaluate_static function to statically evaluate the Window_Kmeans object, calculate multiple metrics, including mean spacing, precision, recall, and F1 score, and obtain 100 data points from the data stream for evaluation
evaluate_static(Window_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
DStream_Kmeans =
DSC_TwoStage(
micro = DSC_DStream(gridsize = 0.1),
macro = DSC_Kmeans(k = 5)
)
update(DStream_Kmeans, stream, n = 500)
DStream_Kmeans
plot(DStream_Kmeans, stream)
evaluate_static(DStream_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
DStream_Kmeans =
DSC_TwoStage(
micro = DSC_DStream(gridsize = 0.1),
macro = DSC_Kmeans(k = 5)
)
update(DStream_Kmeans, stream, n = 500)
DStream_Kmeans
plot(DStream_Kmeans, stream)
evaluate_static(DStream_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
load("rdmTweets-201306.RData")
library(stringr)
preprocessed_text_list <- list()
for (tweet in tweets) {
# Get the url field in tweet
url <- tweet$getText()
# Step 1: Remove URLs from the url field
url <- str_replace_all(url, "http[s]?://\\S+", "")
# Step 2: Convert to lowercase
url <- tolower(url)
# Step 3: Keep only English words and spaces
url <- str_replace_all(url, "[^a-z ]", "")
# Add the preprocessed url field back to the tweet
tweet$setText(url)
# Add preprocessed tweets to the new list
preprocessed_text_list <- c(preprocessed_text_list, list(tweet$getText()))
}
data_frequency <- sum(str_count(preprocessed_text_list, "\\bdata\\b"))
# Count the frequency of the word "mining"
mining_frequency <- sum(str_count(preprocessed_text_list, "\\bmining\\b"))
# Print the results
cat("Frequency of 'data':", data_frequency, "\n")
cat("Frequency of 'mining':", mining_frequency, "\n")
library(tm)
#Create a corpus
corpus <- Corpus(VectorSource(preprocessed_text_list))
# Perform text preprocessing
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#Create document-word matrix
dtm <- DocumentTermMatrix(corpus)
library(wordcloud)
#Create a word cloud
word_freq <- colSums(as.matrix(dtm))
wordcloud(names(word_freq), word_freq, scale = c(3, 0.2), min.freq = 1, colors = brewer.pal(8, "Dark2"))
library("topicmodels")
# Define the number of topics (k) and a random seed (SEED) for reproducibility
k <- 8
SEED <- 123
# Perform LDA on the document-term matrix (dtm) with specified parameters
tweets_TM <- LDA(dtm, k = k, method = "Gibbs", control =
list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))
# Extract the top 6 terms associated with each topic
Terms <- terms(tweets_TM, 6)
Terms
library("stream")
stream <- DSD_Gaussians(k = 4, d = 2, noise = .05)
set.seed(123)
#Create a Reservoir_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Reservoir_Kmeans = DSC_TwoStage(micro = DSC_Sample(k = 200), macro = DSC_Kmeans(k = 5))
# Update the Reservoir_Kmeans object to obtain 500 data points from the data stream for analysis
update(Reservoir_Kmeans, stream, n = 500)
# Print the Reservoir_Kmeans object to view the clustering results
Reservoir_Kmeans
# Draw clustering visualization of Reservoir_Kmeans objects
plot(Reservoir_Kmeans, stream)
# Perform static evaluation on the Reservoir_Kmeans object, calculate multiple metrics (including average distance, precision, recall, F1 score), and obtain 100 data points from the data stream for evaluation
evaluate_static(Reservoir_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
#Create a Window_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Window_Kmeans = DSC_TwoStage(micro = DSC_Window(horizon = 200), macro = DSC_Kmeans(k = 5))
# Get 500 data points from the data stream through the update function and use them to update the Window_Kmeans object
update(Window_Kmeans, stream, n = 500)
#Print the Window_Kmeans object to view the clustering results
Window_Kmeans
# Draw clustering visualization of Window_Kmeans objects
plot(Window_Kmeans, stream)
# Use the evaluate_static function to statically evaluate the Window_Kmeans object, calculate multiple metrics, including mean spacing, precision, recall, and F1 score, and obtain 100 data points from the data stream for evaluation
evaluate_static(Window_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
DStream_Kmeans =
DSC_TwoStage(
micro = DSC_DStream(gridsize = 0.1),
macro = DSC_Kmeans(k = 5)
)
update(DStream_Kmeans, stream, n = 500)
DStream_Kmeans
plot(DStream_Kmeans, stream)
evaluate_static(DStream_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
library(sf)
library(ggplot2)
library(rnaturalearth)
library(rnaturalearthdata)
require(maps)
data(world.cities)
australian_cities <- world.cities[world.cities$country.etc == "Australia", ]
head(australian_cities)
require(maps)
require(terra)
data(world.cities)
# Create a sample dataset of cities with their coordinates
australian_cities_small <- australian_cities[australian_cities$pop <= 1000000,c("name", "lat", "long")]
australian_cities_large <- australian_cities[australian_cities$pop > 1000000,c("name", "lat", "long")]
# Create an sf data frame from the cities dataset
cities_sf_small <- st_as_sf(australian_cities_small, coords = c("long", "lat"))
cities_sf_large <- st_as_sf(australian_cities_large, coords = c("long", "lat"))
# Plot the map of Australia
# plot(world)
terra::plot(ne_states(geounit = "australia"))
# Add city locations as dots
plot(cities_sf_small, pch = 16, col = "blue", cex = 0.1, add = TRUE)
plot(cities_sf_large, pch = 16, col = "red", cex = 1, add = TRUE)
aus_shape <- vect("SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp")
aus_shape <- aus_shape[!is.na(aus_shape$AREASQKM21)]
library(RColorBrewer)
sa_shape <- aus_shape[aus_shape$STE_NAME21 == "South Australia"]
sa_shape_4 <- aggregate(sa_shape, by = 'SA4_NAME21')
par(mar = c(1, 1, 1, 1))
options(repr.plot.width = 20, repr.plot.height = 20)
color_setting <- brewer.pal(length(unique(sa_shape_4$SA4_NAME21)), "Set1")
plot(sa_shape_4, col = color_setting)
plot(sa_shape, add = TRUE)
legend("top", legend = unique(sa_shape_4$SA4_NAME21), fill = color_setting, title = "statistical areas level 4", cex = 0.75)
library(terra)
greater_adelaide <- aus_shape[aus_shape$GCC_NAME21 == "Greater Adelaide"]
plot(greater_adelaide)
# Load crime data
crime_data <- read.csv('crimeCounts.csv')
nrow(crime_data)
crime_data <- na.omit(crime_data)
nrow(crime_data)
colnames(crime_data) <- c('id', 'suburb', 'crimeCounts')
colnames(crime_data)
salisbury <- sa_shape[sa_shape$SA3_NAME21 == "Salisbury"]
suburb_crime_count <- left_join(
data.frame("suburb" = tolower(salisbury$SA2_NAME21)),
crime_data %>% mutate(suburb = tolower(suburb)),
by = "suburb"
)
salisbury$crimeCounts <- suburb_crime_count$crimeCounts
salisbury_raster <- rast(
ncol = 1000, nrow = 1000,
xmin = 138.50, xmax = 138.71,
ymin = -34.84, ymax = -34.69
)
salisbury_raster <- rasterize(salisbury, salisbury_raster, "crimeCounts")
terra::plot(
salisbury_raster,
col = brewer.pal(10, "Reds")
)
terra::lines(salisbury)
text(salisbury, salisbury$SA2_NAME21, inside = TRUE, cex = 0.6, col = "Black")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
load("rdmTweets-201306.RData")
library(stringr)
preprocessed_text_list <- list()
for (tweet in tweets) {
# Get the url field in tweet
url <- tweet$getText()
# Step 1: Remove URLs from the url field
url <- str_replace_all(url, "http[s]?://\\S+", "")
# Step 2: Convert to lowercase
url <- tolower(url)
# Step 3: Keep only English words and spaces
url <- str_replace_all(url, "[^a-z ]", "")
# Add the preprocessed url field back to the tweet
tweet$setText(url)
# Add preprocessed tweets to the new list
preprocessed_text_list <- c(preprocessed_text_list, list(tweet$getText()))
}
data_frequency <- sum(str_count(preprocessed_text_list, "\\bdata\\b"))
# Count the frequency of the word "mining"
mining_frequency <- sum(str_count(preprocessed_text_list, "\\bmining\\b"))
# Print the results
cat("Frequency of 'data':", data_frequency, "\n")
cat("Frequency of 'mining':", mining_frequency, "\n")
library(tm)
#Create a corpus
corpus <- Corpus(VectorSource(preprocessed_text_list))
# Perform text preprocessing
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#Create document-word matrix
dtm <- DocumentTermMatrix(corpus)
library(wordcloud)
#Create a word cloud
word_freq <- colSums(as.matrix(dtm))
wordcloud(names(word_freq), word_freq, scale = c(3, 0.2), min.freq = 1, colors = brewer.pal(8, "Dark2"))
library("topicmodels")
# Define the number of topics (k) and a random seed (SEED) for reproducibility
k <- 8
SEED <- 123
# Perform LDA on the document-term matrix (dtm) with specified parameters
tweets_TM <- LDA(dtm, k = k, method = "Gibbs", control =
list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))
# Extract the top 6 terms associated with each topic
Terms <- terms(tweets_TM, 6)
Terms
library("stream")
stream <- DSD_Gaussians(k = 4, d = 2, noise = .05)
set.seed(123)
#Create a Reservoir_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Reservoir_Kmeans = DSC_TwoStage(micro = DSC_Sample(k = 200), macro = DSC_Kmeans(k = 5))
# Update the Reservoir_Kmeans object to obtain 500 data points from the data stream for analysis
update(Reservoir_Kmeans, stream, n = 500)
# Print the Reservoir_Kmeans object to view the clustering results
Reservoir_Kmeans
# Draw clustering visualization of Reservoir_Kmeans objects
plot(Reservoir_Kmeans, stream)
# Perform static evaluation on the Reservoir_Kmeans object, calculate multiple metrics (including average distance, precision, recall, F1 score), and obtain 100 data points from the data stream for evaluation
evaluate_static(Reservoir_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
#Create a Window_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Window_Kmeans = DSC_TwoStage(micro = DSC_Window(horizon = 200), macro = DSC_Kmeans(k = 5))
# Get 500 data points from the data stream through the update function and use them to update the Window_Kmeans object
update(Window_Kmeans, stream, n = 500)
#Print the Window_Kmeans object to view the clustering results
Window_Kmeans
# Draw clustering visualization of Window_Kmeans objects
plot(Window_Kmeans, stream)
# Use the evaluate_static function to statically evaluate the Window_Kmeans object, calculate multiple metrics, including mean spacing, precision, recall, and F1 score, and obtain 100 data points from the data stream for evaluation
evaluate_static(Window_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
DStream_Kmeans =
DSC_TwoStage(
micro = DSC_DStream(gridsize = 0.1),
macro = DSC_Kmeans(k = 5)
)
update(DStream_Kmeans, stream, n = 500)
DStream_Kmeans
plot(DStream_Kmeans, stream)
evaluate_static(DStream_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
library(sf)
library(ggplot2)
library(rnaturalearth)
library(rnaturalearthdata)
require(maps)
data(world.cities)
australian_cities <- world.cities[world.cities$country.etc == "Australia", ]
head(australian_cities)
require(maps)
require(terra)
data(world.cities)
# Create a sample dataset of cities with their coordinates
australian_cities_small <- australian_cities[australian_cities$pop <= 1000000,c("name", "lat", "long")]
australian_cities_large <- australian_cities[australian_cities$pop > 1000000,c("name", "lat", "long")]
# Create an sf data frame from the cities dataset
cities_sf_small <- st_as_sf(australian_cities_small, coords = c("long", "lat"))
cities_sf_large <- st_as_sf(australian_cities_large, coords = c("long", "lat"))
# Plot the map of Australia
# plot(world)
terra::plot(ne_states(geounit = "australia"))
# Add city locations as dots
plot(cities_sf_small, pch = 16, col = "blue", cex = 0.1, add = TRUE)
plot(cities_sf_large, pch = 16, col = "red", cex = 1, add = TRUE)
aus_shape <- vect("SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp")
aus_shape <- aus_shape[!is.na(aus_shape$AREASQKM21)]
library(RColorBrewer)
sa_shape <- aus_shape[aus_shape$STE_NAME21 == "South Australia"]
sa_shape_4 <- aggregate(sa_shape, by = 'SA4_NAME21')
par(mar = c(1, 1, 1, 1))
options(repr.plot.width = 20, repr.plot.height = 20)
color_setting <- brewer.pal(length(unique(sa_shape_4$SA4_NAME21)), "Set1")
plot(sa_shape_4, col = color_setting)
plot(sa_shape, add = TRUE)
legend("top", legend = unique(sa_shape_4$SA4_NAME21), fill = color_setting, title = "statistical areas level 4", cex = 0.75)
library(terra)
greater_adelaide <- aus_shape[aus_shape$GCC_NAME21 == "Greater Adelaide"]
plot(greater_adelaide)
# Load crime data
crime_data <- read.csv('crimeCounts.csv')
nrow(crime_data)
crime_data <- na.omit(crime_data)
nrow(crime_data)
colnames(crime_data) <- c('id', 'suburb', 'crimeCounts')
colnames(crime_data)
salisbury <- sa_shape[sa_shape$SA3_NAME21 == "Salisbury"]
suburb_crime_count <- left_join(
data.frame("suburb" = tolower(salisbury$SA2_NAME21)),
crime_data %>% mutate(suburb = tolower(suburb)),
by = "suburb"
)
salisbury$crimeCounts <- suburb_crime_count$crimeCounts
salisbury_raster <- rast(
ncol = 1000, nrow = 1000,
xmin = 138.50, xmax = 138.71,
ymin = -34.84, ymax = -34.69
)
salisbury_raster <- rasterize(salisbury, salisbury_raster, "crimeCounts")
terra::plot(
salisbury_raster,
col = brewer.pal(10, "Reds")
)
terra::lines(salisbury)
text(salisbury, salisbury$SA2_NAME21, inside = TRUE, cex = 0.6, col = "Black")
gc()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
load("rdmTweets-201306.RData")
library(stringr)
preprocessed_text_list <- list()
for (tweet in tweets) {
# Get the url field in tweet
url <- tweet$getText()
# Step 1: Remove URLs from the url field
url <- str_replace_all(url, "http[s]?://\\S+", "")
# Step 2: Convert to lowercase
url <- tolower(url)
# Step 3: Keep only English words and spaces
url <- str_replace_all(url, "[^a-z ]", "")
# Add the preprocessed url field back to the tweet
tweet$setText(url)
# Add preprocessed tweets to the new list
preprocessed_text_list <- c(preprocessed_text_list, list(tweet$getText()))
}
data_frequency <- sum(str_count(preprocessed_text_list, "\\bdata\\b"))
# Count the frequency of the word "mining"
mining_frequency <- sum(str_count(preprocessed_text_list, "\\bmining\\b"))
# Print the results
cat("Frequency of 'data':", data_frequency, "\n")
cat("Frequency of 'mining':", mining_frequency, "\n")
library(tm)
#Create a corpus
corpus <- Corpus(VectorSource(preprocessed_text_list))
# Perform text preprocessing
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
#Create document-word matrix
dtm <- DocumentTermMatrix(corpus)
library(wordcloud)
#Create a word cloud
word_freq <- colSums(as.matrix(dtm))
wordcloud(names(word_freq), word_freq, scale = c(3, 0.2), min.freq = 1, colors = brewer.pal(8, "Dark2"))
library("topicmodels")
# Define the number of topics (k) and a random seed (SEED) for reproducibility
k <- 8
SEED <- 123
# Perform LDA on the document-term matrix (dtm) with specified parameters
tweets_TM <- LDA(dtm, k = k, method = "Gibbs", control =
list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))
# Extract the top 6 terms associated with each topic
Terms <- terms(tweets_TM, 6)
Terms
library("stream")
stream <- DSD_Gaussians(k = 4, d = 2, noise = .05)
set.seed(123)
#Create a Reservoir_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Reservoir_Kmeans = DSC_TwoStage(micro = DSC_Sample(k = 200), macro = DSC_Kmeans(k = 5))
# Update the Reservoir_Kmeans object to obtain 500 data points from the data stream for analysis
update(Reservoir_Kmeans, stream, n = 500)
# Print the Reservoir_Kmeans object to view the clustering results
Reservoir_Kmeans
# Draw clustering visualization of Reservoir_Kmeans objects
plot(Reservoir_Kmeans, stream)
# Perform static evaluation on the Reservoir_Kmeans object, calculate multiple metrics (including average distance, precision, recall, F1 score), and obtain 100 data points from the data stream for evaluation
evaluate_static(Reservoir_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
#Create a Window_Kmeans object and use DSC_TwoStage to perform two-stage cluster analysis
Window_Kmeans = DSC_TwoStage(micro = DSC_Window(horizon = 200), macro = DSC_Kmeans(k = 5))
# Get 500 data points from the data stream through the update function and use them to update the Window_Kmeans object
update(Window_Kmeans, stream, n = 500)
#Print the Window_Kmeans object to view the clustering results
Window_Kmeans
# Draw clustering visualization of Window_Kmeans objects
plot(Window_Kmeans, stream)
# Use the evaluate_static function to statically evaluate the Window_Kmeans object, calculate multiple metrics, including mean spacing, precision, recall, and F1 score, and obtain 100 data points from the data stream for evaluation
evaluate_static(Window_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
set.seed(123)
DStream_Kmeans =
DSC_TwoStage(
micro = DSC_DStream(gridsize = 0.1),
macro = DSC_Kmeans(k = 5)
)
update(DStream_Kmeans, stream, n = 500)
DStream_Kmeans
plot(DStream_Kmeans, stream)
evaluate_static(DStream_Kmeans, stream, measure = c("average.between", "precision", "recall", "F1"), n = 100)
library(sf)
library(ggplot2)
library(rnaturalearth)
library(rnaturalearthdata)
require(maps)
data(world.cities)
australian_cities <- world.cities[world.cities$country.etc == "Australia", ]
head(australian_cities)
require(maps)
require(terra)
data(world.cities)
# Create a sample dataset of cities with their coordinates
australian_cities_small <- australian_cities[australian_cities$pop <= 1000000,c("name", "lat", "long")]
australian_cities_large <- australian_cities[australian_cities$pop > 1000000,c("name", "lat", "long")]
# Create an sf data frame from the cities dataset
cities_sf_small <- st_as_sf(australian_cities_small, coords = c("long", "lat"))
cities_sf_large <- st_as_sf(australian_cities_large, coords = c("long", "lat"))
# Plot the map of Australia
# plot(world)
terra::plot(ne_states(geounit = "australia"))
# Add city locations as dots
plot(cities_sf_small, pch = 16, col = "blue", cex = 0.1, add = TRUE)
plot(cities_sf_large, pch = 16, col = "red", cex = 1, add = TRUE)
aus_shape <- vect("SA2_2021_AUST_SHP_GDA2020/SA2_2021_AUST_GDA2020.shp")
aus_shape <- aus_shape[!is.na(aus_shape$AREASQKM21)]
library(RColorBrewer)
sa_shape <- aus_shape[aus_shape$STE_NAME21 == "South Australia"]
sa_shape_4 <- aggregate(sa_shape, by = 'SA4_NAME21')
par(mar = c(1, 1, 1, 1))
options(repr.plot.width = 20, repr.plot.height = 20)
color_setting <- brewer.pal(length(unique(sa_shape_4$SA4_NAME21)), "Set1")
plot(sa_shape_4, col = color_setting)
plot(sa_shape, add = TRUE)
legend("top", legend = unique(sa_shape_4$SA4_NAME21), fill = color_setting, title = "statistical areas level 4", cex = 0.75)
library(terra)
greater_adelaide <- aus_shape[aus_shape$GCC_NAME21 == "Greater Adelaide"]
plot(greater_adelaide)
# Load crime data
crime_data <- read.csv('crimeCounts.csv')
nrow(crime_data)
crime_data <- na.omit(crime_data)
nrow(crime_data)
colnames(crime_data) <- c('id', 'suburb', 'crimeCounts')
colnames(crime_data)
salisbury <- sa_shape[sa_shape$SA3_NAME21 == "Salisbury"]
suburb_crime_count <- left_join(
data.frame("suburb" = tolower(salisbury$SA2_NAME21)),
crime_data %>% mutate(suburb = tolower(suburb)),
by = "suburb"
)
salisbury$crimeCounts <- suburb_crime_count$crimeCounts
salisbury_raster <- rast(
ncol = 1000, nrow = 1000,
xmin = 138.50, xmax = 138.71,
ymin = -34.84, ymax = -34.69
)
salisbury_raster <- rasterize(salisbury, salisbury_raster, "crimeCounts")
terra::plot(
salisbury_raster,
col = brewer.pal(10, "Reds")
)
terra::lines(salisbury)
text(salisbury, salisbury$SA2_NAME21, inside = TRUE, cex = 0.6, col = "Black")
