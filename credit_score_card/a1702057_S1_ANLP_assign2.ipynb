{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8ZyI0beWPQHF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Read the dataset\n",
    "news_df = pd.read_csv('news_dataset.csv', encoding='latin1')\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "news_df['cleaned_article'] = news_df['article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PL2u2ztuPSuG",
    "outputId": "3431cb9e-d4a7-40e2-90db-91c3e22ad4b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the pre-trained QA model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IO1GDNCTPT_C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def answer_question(question, article):\n",
    "    # Encode the question and article\n",
    "    encoded_input = tokenizer(question, article, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    # Get the model output\n",
    "    output = model(**encoded_input)\n",
    "    # Extract the answer and confidence scores\n",
    "    answer_start = output.start_logits.argmax()\n",
    "    answer_end = output.end_logits.argmax()\n",
    "    answer = tokenizer.decode(encoded_input[\"input_ids\"][0][answer_start:answer_end+1])\n",
    "    # Set confidence threshold\n",
    "    confidence_threshold = 0.5\n",
    "    # Calculate the confidence score\n",
    "    start_prob = torch.softmax(output.start_logits, dim=1)[0][answer_start].item()\n",
    "    end_prob = torch.softmax(output.end_logits, dim=1)[0][answer_end].item()\n",
    "    confidence_score = (start_prob + end_prob) / 2\n",
    "    if confidence_score >= confidence_threshold:\n",
    "        return answer\n",
    "    else:\n",
    "        return \"No answer found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEN1NjjcPYe-",
    "outputId": "f33bde5e-b5b3-48ce-b100-16e455690ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung\n",
      "Article ID: 17307\n",
      "Answer: No answer found.\n"
     ]
    }
   ],
   "source": [
    "# Example test\n",
    "question = \"Who is the vice chairman of Samsung\"\n",
    "article_id = 17307\n",
    "article = news_df[news_df['id'] == article_id]['cleaned_article'].iloc[0]\n",
    "answer = answer_question(question, article)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Article ID: {article_id}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vTcGBtNPtz8",
    "outputId": "e74bbb15-46be-4995-a741-cfef32950e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the President during the conflict?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: George W. Bush\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is the Senator of Colorado?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Cory Gardner\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What was the revolt?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Tea Party Revolt\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did they get control back?\n",
      "Predicted Answer: 2006\n",
      "True Answer: 2010\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Where is the senior Republican from?\n",
      "Predicted Answer: oklahoma\n",
      "True Answer: Oklahoma\n",
      "Result: Correct\n",
      "\n",
      "Question: Who was the president during the Iraq War?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: George W. Bush\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What amount did Fox News offer?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: 20 Million\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Where did Charlie Rose interview Kelly?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: CBS Sunday Morning\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did Andrew Lack take over?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: 2015\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What is Andrew Lack adding?\n",
      "Predicted Answer: a journalist\n",
      "True Answer: journalist schooled in the preferences and worldviews of the conservative Americans who helped elect Mr Trump\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When is Donald Trump inaugurated?\n",
      "Predicted Answer: jan 20\n",
      "True Answer: Jan. 20\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is a executive chairman?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Rupert Murdoch\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is an executive chairman of 21st Century Fox?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Rupert Murdoch\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is a novelist?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Douglas Brunt\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Where was the shooting?\n",
      "Predicted Answer: panzhihua\n",
      "True Answer: Panzhihua\n",
      "Result: Correct\n",
      "\n",
      "Question: Who was embarrassed by the violence?\n",
      "Predicted Answer: president xi jinping\n",
      "True Answer: Xi Jinping\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Chen Zhongshu is the head of what?\n",
      "Predicted Answer: panzhihua land and resources bureau\n",
      "True Answer: Panzhihua Land and Resources Bureau,\n",
      "Result: Incorrect\n",
      "\n",
      "Question: The shooting happened where?\n",
      "Predicted Answer: panzhihua\n",
      "True Answer: Panzhihua\n",
      "Result: Correct\n",
      "\n",
      "Question: What was Chen Zhongshu the head of?\n",
      "Predicted Answer: panzhihua land and resources bureau\n",
      "True Answer: Panzhihua Land and Resources Bureau,\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did Zhang starting working there?\n",
      "Predicted Answer: 2006\n",
      "True Answer: 2006\n",
      "Result: Correct\n",
      "\n",
      "Question: Where is Panzhihua?\n",
      "Predicted Answer: an industrial city in sichuan province\n",
      "True Answer: Sichuan Province\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What is Panzhihua?\n",
      "Predicted Answer: an industrial city in sichuan province\n",
      "True Answer: industrial city\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When where they murdered?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: June 2015\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What did they find him guilty of?\n",
      "Predicted Answer: hate crimes resulting in death obstruction of religion and firearms violations\n",
      "True Answer: hate crimes resulting in death, obstruction of religion and firearms violations\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who faces the death penalty?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Dylan Roof\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who faces life in prison?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Dylan Roof\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did he write?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Dec. 16\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is evil?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Dylan Roof\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did the police officer shoot?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: 2015\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When was the coalition made?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: 2014\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What did the Islamic State seize?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: large areas in Iraq and neighboring Syria\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did the car bombing happen?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Monday\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Where did the bombing occur?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: a busy Baghdad market\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Where did the attack occur?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Sadr City\n",
      "Result: Incorrect\n",
      "\n",
      "Question: How many died?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: 36\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What was repeatedly attacked after the invasion?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: Sadr City\n",
      "Result: Incorrect\n",
      "\n",
      "Question: What did the gunman have?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: a rifle\n",
      "Result: Incorrect\n",
      "\n",
      "Question: Who is the spokesman?\n",
      "Predicted Answer: numan kurtulmus\n",
      "True Answer: Numan Kurtulmus\n",
      "Result: Correct\n",
      "\n",
      "Question: Where is the gunman from?\n",
      "Predicted Answer: kyrgyzstan or elsewhere in central asia\n",
      "True Answer: Kyrgyzstan or elsewhere in Central Asia\n",
      "Result: Correct\n",
      "\n",
      "Question: What happened in November?\n",
      "Predicted Answer: No answer found.\n",
      "True Answer: A car bombing\n",
      "Result: Incorrect\n",
      "\n",
      "Question: When did Kurtulmus say the attack happened?\n",
      "Predicted Answer: after midnight on sunday morning\n",
      "True Answer: just after midnight on Sunday morning\n",
      "Result: Incorrect\n",
      "\n",
      "Accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Read test questions and answers\n",
    "test_data = []\n",
    "with open('test_questions_students_contributed.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        match = re.search(r\"\\('(.*?)', (\\d+)\\) \\('(.*?)', \\d+\\)\", line)\n",
    "        if match:\n",
    "            question = match.group(1)\n",
    "            article_id = int(match.group(2))\n",
    "            answer = match.group(3)\n",
    "            test_data.append((question, article_id, answer))\n",
    "\n",
    "# Evaluate the QA system\n",
    "correct_answers = 0\n",
    "for question, article_id, true_answer in test_data:\n",
    "    article = news_df[news_df['id'] == article_id]['cleaned_article'].iloc[0]\n",
    "    predicted_answer = answer_question(question, article)\n",
    "    # Print question, predicted answer, and true answer\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"True Answer: {true_answer}\")\n",
    "    if predicted_answer.lower() == true_answer.lower():\n",
    "        correct_answers += 1\n",
    "        print(\"Result: Correct\\n\")\n",
    "    else:\n",
    "        print(\"Result: Incorrect\\n\")\n",
    "\n",
    "accuracy = correct_answers / len(test_data)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
